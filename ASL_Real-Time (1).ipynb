{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-14 15:43:24.979780: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-02-14 15:43:27.499406: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nithin/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-02-14 15:43:27.499525: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-02-14 15:43:34.770869: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nithin/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-02-14 15:43:34.771614: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/nithin/.local/lib/python3.10/site-packages/cv2/../../lib64:\n",
      "2023-02-14 15:43:34.771643: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'load_img' from 'keras.preprocessing.image' (/home/nithin/.local/lib/python3.10/site-packages/keras/preprocessing/image.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36003/1742129634.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'load_img' from 'keras.preprocessing.image' (/home/nithin/.local/lib/python3.10/site-packages/keras/preprocessing/image.py)"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from keras.models import load_model\n",
    "from keras.preprocessing.image import load_img, img_to_array\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"asl_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_dict = {0:'0', \n",
    "                 1:'A', \n",
    "                 2:'B', \n",
    "                 3:'C', \n",
    "                 4:'D', \n",
    "                 5:'E',\n",
    "                 6:'F',\n",
    "                 7:'G',\n",
    "                 8:'H',\n",
    "                 9:'I',\n",
    "                 10:'J',\n",
    "                 11:'K',\n",
    "                 12:'L',\n",
    "                 13:'M',\n",
    "                 14:'N',\n",
    "                 15:'O',\n",
    "                 16:'P',\n",
    "                 17:\"Q\",\n",
    "                 18:'R',\n",
    "                 19:'S',\n",
    "                 20:'T', \n",
    "                 21:'U', \n",
    "                 22:'V',\n",
    "                 23:'W',\n",
    "                 24:'X',\n",
    "                 25:'Y',\n",
    "                 26:'Z'}\n",
    "color_dict=(0,255,0)\n",
    "x=0\n",
    "y=0\n",
    "w=64\n",
    "h=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Real-Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=128\n",
    "minValue = 70\n",
    "source=cv2.VideoCapture(0)\n",
    "count = 0\n",
    "string = \" \"\n",
    "prev = \" \"\n",
    "prev_val = 0\n",
    "while(True):\n",
    "    ret,img=source.read()\n",
    "    gray=cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    #cv2.rectangle(img,(x,y),(x+w,y+h),color_dict,2)\n",
    "    cv2.rectangle(img,(24,24),(250 , 250),color_dict,2)\n",
    "    crop_img=gray[24:250,24:250]\n",
    "    count = count + 1\n",
    "    if(count % 100 == 0):\n",
    "        prev_val = count\n",
    "    cv2.putText(img, str(prev_val//100), (300, 150),cv2.FONT_HERSHEY_SIMPLEX,1.5,(255,255,255),2) \n",
    "    blur = cv2.GaussianBlur(crop_img,(5,5),2)\n",
    "    th3 = cv2.adaptiveThreshold(blur,255,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,cv2.THRESH_BINARY_INV,11,2)\n",
    "    ret, res = cv2.threshold(th3, minValue, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
    "    resized=cv2.resize(res,(img_size,img_size))\n",
    "    normalized=resized/255.0\n",
    "    reshaped=np.reshape(normalized,(1,img_size,img_size,1))\n",
    "    result = model.predict(reshaped)\n",
    "    #print(result)\n",
    "    label=np.argmax(result,axis=1)[0]\n",
    "    if(count == 300):\n",
    "        count = 99\n",
    "        prev= labels_dict[label] \n",
    "        if(label == 0):\n",
    "               string = string + \" \"\n",
    "            #if(len(string)==1 or string[len(string)] != \" \"):\n",
    "             \n",
    "        else:\n",
    "                string = string + prev\n",
    "    \n",
    "    cv2.putText(img, prev, (24, 14),cv2.FONT_HERSHEY_SIMPLEX,0.8,(255,255,255),2) \n",
    "    cv2.putText(img, string, (275, 50),cv2.FONT_HERSHEY_SIMPLEX,0.8,(200,200,200),2)\n",
    "    cv2.imshow(\"Gray\",res)    \n",
    "    cv2.imshow('LIVE',img)\n",
    "    key=cv2.waitKey(1)\n",
    "    \n",
    " \n",
    "    if(key==27):#press Esc. to exit\n",
    "        break\n",
    "print(string)        \n",
    "cv2.destroyAllWindows()\n",
    "source.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install gTTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gtts import gTTS \n",
    "  \n",
    "# This module is imported so that we can  \n",
    "# play the converted audio \n",
    "import os \n",
    "  \n",
    "# The text that you want to convert to audio \n",
    "  \n",
    "# Language in which you want to convert \n",
    "language = 'en'\n",
    "# Passing the text and language to the engine,  \n",
    "# here we have marked slow=False. Which tells  \n",
    "# the module that the converted audio should  \n",
    "# have a high speed \n",
    "myobj = gTTS(text=string, lang=language, slow=False) \n",
    "  \n",
    "# Saving the converted audio in a mp3 file named \n",
    "# welcome  \n",
    "myobj.save(\"welcome2121.mp3\") \n",
    "  \n",
    "# Playing the converted file \n",
    "os.system(\"welcome.mp3\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from playsound import playsound\n",
    "playsound('welcome2121.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
